{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65bdc26b",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"padding:18px;border:2px solid #e8eaed;border-radius:12px;background:#fcfcff\">\n",
    "  <h1 style=\"margin:0;color:#1a73e8\">NLP Disaster Tweets — Week 4 Mini‑Project (BiLSTM)</h1>\n",
    "  <p style=\"margin:6px 0 0 0;color:#5f6368\">\n",
    "    A tidy, single‑family approach: <b>Tokenizer → Embedding → Bidirectional LSTM</b> with a small, staged HPO.\n",
    "  </p>\n",
    "</div>\n",
    "\n",
    "<details style=\"margin-top:8px\">\n",
    "  <summary><b>Roadmap</b> (click)</summary>\n",
    "  <ol>\n",
    "    <li>Brief description (problem & data)</li>\n",
    "    <li>EDA → cleaning decisions</li>\n",
    "    <li>Model architecture (BiLSTM) & rationale</li>\n",
    "    <li>Experiments & small HPO</li>\n",
    "    <li>Results, analysis & conclusion</li>\n",
    "    <li>Submission & references</li>\n",
    "  </ol>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622987c6",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Brief description of the problem and data \n",
    "\n",
    "**Description of the Problem and NLP Context**\n",
    "\n",
    "This project is based on the Kaggle *Natural Language Processing with Disaster Tweets* competition, where the task is to automatically classify short text messages (tweets) as either describing a real disaster (`target = 1`) or not related to a disaster (`target = 0`).\n",
    "\n",
    "**Dataset Overview**\n",
    "- **Train set**: ~7,600 tweets, each with an ID, text content, optional keyword, optional location, and the binary target.  \n",
    "- **Test set**: ~3,300 tweets, same structure without the target.  \n",
    "- Tweets are short (median length ~15–20 tokens) and written in informal, noisy language with abbreviations, hashtags, mentions, and URLs.\n",
    "\n",
    "This problem falls under the field of **Natural Language Processing (NLP)**, which involves techniques for processing and analyzing human language so that computers can understand and act on it. Specifically, it is a **binary text classification** problem.\n",
    "\n",
    "---\n",
    "\n",
    "**Project Plan Overview**\n",
    "\n",
    "We will:\n",
    "1. **Explore and clean the data** – Inspect text lengths, class balance, and patterns; apply minimal cleaning (lowercase, replace URLs/mentions, keep hashtag words).  \n",
    "2. **Convert text to numeric form** – Use tokenization and a learned embedding layer to map words into dense vectors suitable for neural networks.  \n",
    "3. **Build and train a sequential neural network** – Main model: Bidirectional LSTM, chosen to capture both past and future context in a tweet.  \n",
    "4. **Hyperparameter tuning** – Run a small, staged search over sequence length, pooling type, hidden units, learning rate, and dropout to find the best-performing setup.  \n",
    "5. **Evaluate and analyze results** – Compare model variants by AUC (primary metric) and accuracy (secondary); discuss what improved performance and what didn’t.  \n",
    "6. **Produce deliverables** – Submit predictions to Kaggle, include a leaderboard screenshot, and push the cleaned notebook to a public GitHub repository.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6734fd",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Setup — imports, config, helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b07a4736",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 14:55:02.058164: I tensorflow/core/util/port.cc:111] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-12 14:55:02.100536: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-08-12 14:55:02.100564: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-08-12 14:55:02.100589: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-08-12 14:55:02.108621: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.14.0\n",
      "Num GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os, re, random, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score  # + f1\n",
    "\n",
    "# Style (optional)\n",
    "from IPython.display import display, HTML\n",
    "def h2(txt): display(HTML(f'<h2 style=\"color:#1a73e8\">{txt}</h2>'))\n",
    "def note(msg): display(HTML(f'<div style=\"padding:10px;border-left:4px solid #1a73e8;background:#f1f5ff\">{msg}</div>'))\n",
    "\n",
    "# Reproducibility\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "# Config\n",
    "CFG = {\n",
    "    \"seed\": 42,\n",
    "    \"subset_rows\": 0,         # 0 = use all; >0 = use only first N rows for quick local tests\n",
    "    \"val_size\": 0.15,         # validation split from train\n",
    "    \"max_len\": 50,            # will be tuned in HPO\n",
    "    \"batch_size\": 32,\n",
    "    \"epochs\": 6,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"embedding_dim\": 100,\n",
    "    \"units\": 64,\n",
    "    \"dropout\": 0.2,\n",
    "    \"paths\": {\n",
    "        \"train\": \"data/train.csv\",\n",
    "        \"test\": \"data/test.csv\",\n",
    "        \"submission\": \"submissions/sub.csv\"\n",
    "    },\n",
    "    \"metric\": \"f1\",                       # Kaggle leaderboard metric\n",
    "    \"kaggle_competition\": \"nlp-getting-started\"\n",
    "}\n",
    "\n",
    "# Create dirs, set seed\n",
    "os.makedirs(os.path.dirname(CFG[\"paths\"][\"submission\"]), exist_ok=True)\n",
    "set_seed(CFG[\"seed\"])\n",
    "\n",
    "# Determinism + safer GPU memory handling (best-effort)\n",
    "try:\n",
    "    tf.config.experimental.enable_op_determinism()\n",
    "except Exception:\n",
    "    pass\n",
    "_gpus = tf.config.list_physical_devices('GPU')\n",
    "for _g in _gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_memory_growth(_g, True)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "print(\"TensorFlow:\", tf.__version__)\n",
    "print(\"Num GPUs:\", len(_gpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d85376-8ad1-44a8-a9e2-c43b724fdf42",
   "metadata": {},
   "source": [
    "### Download the data from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "851ff68b-29e3-4684-b424-80147ae45a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading competition data to data...\n",
      "Downloading nlp-getting-started.zip to data\n",
      "\n",
      "Archive:  data/nlp-getting-started.zip\n",
      "  inflating: data/sample_submission.csv  \n",
      "  inflating: data/test.csv           \n",
      "  inflating: data/train.csv          \n",
      "Download complete: ['sample_submission.csv', 'test.csv', 'train.csv']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 593k/593k [00:00<00:00, 46.7MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Kaggle download helper\n",
    "import subprocess, pathlib\n",
    "\n",
    "def download_kaggle_data():\n",
    "    data_dir = pathlib.Path(\"data\")\n",
    "    data_dir.mkdir(exist_ok=True)\n",
    "    print(f\"Downloading competition data to {data_dir}...\")\n",
    "    subprocess.run([\n",
    "        \"kaggle\", \"competitions\", \"download\",\n",
    "        \"-c\", CFG[\"kaggle_competition\"],\n",
    "        \"-p\", str(data_dir)\n",
    "    ], check=True)\n",
    "    subprocess.run([\"unzip\", \"-o\", str(data_dir/f\"{CFG['kaggle_competition']}.zip\"), \"-d\", str(data_dir)], check=True)\n",
    "    print(\"Download complete:\", sorted(p.name for p in data_dir.glob(\"*.csv\")))\n",
    "\n",
    "download_kaggle_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9ae460",
   "metadata": {},
   "source": [
    "\n",
    "## 3) EDA — inspect, visualize, decide cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1279d863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h2 style=\"color:#1a73e8\">Preview</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about #earthquake is different cities, s...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond, geese are...\n",
       "3   9     NaN      NaN           Apocalypse lighting. #Spokane #wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h2 style=\"color:#1a73e8\">Class balance (train)</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    0.57034\n",
       "1    0.42966\n",
       "Name: ratio, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h2 style=\"color:#1a73e8\">Text length</h2>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load\n",
    "train_df = pd.read_csv(CFG[\"paths\"][\"train\"])\n",
    "test_df  = pd.read_csv(CFG[\"paths\"][\"test\"])\n",
    "\n",
    "# Subset for quick local testing\n",
    "if CFG[\"subset_rows\"] and CFG[\"subset_rows\"] > 0:\n",
    "    train_df = train_df.iloc[:CFG[\"subset_rows\"]].copy()\n",
    "    test_df  = test_df.iloc[: min(CFG[\"subset_rows\"], len(test_df))].copy()\n",
    "\n",
    "h2(\"Preview\")\n",
    "display(train_df.head())\n",
    "display(test_df.head())\n",
    "\n",
    "h2(\"Class balance (train)\")\n",
    "if \"target\" in train_df.columns:\n",
    "    display(train_df[\"target\"].value_counts(normalize=True).rename(\"ratio\"))\n",
    "\n",
    "h2(\"Text length\")\n",
    "train_df[\"text_len\"] = train_df[\"text\"].astype(str).str.split().map(len)\n",
    "plt.figure()\n",
    "train_df[\"text_len\"].hist(bins=40)\n",
    "plt.title(\"Text length\")\n",
    "plt.xlabel(\"tokens\")\n",
    "plt.ylabel(\"count\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1703f2ff-11f0-477b-bb73-e160801e409e",
   "metadata": {},
   "source": [
    "## EDA Summary (Train Set)\n",
    "\n",
    "- **Class Balance**\n",
    "  - `target = 0`: ~57%\n",
    "  - `target = 1`: ~43%\n",
    "  - → Modest class imbalance, but not severe enough to require heavy rebalancing techniques.\n",
    "\n",
    "- **Text Length**\n",
    "  - Most tweets are between **10–22 tokens**.\n",
    "  - Median length ≈ 15–16 tokens.\n",
    "  - Very few tweets exceed 30 tokens.\n",
    "\n",
    "- **Keyword & Location Fields**\n",
    "  - High proportion of missing values in both fields.\n",
    "  - `keyword` may contain informative disaster-related terms.\n",
    "  - `location` is noisy and sparse, likely requiring cleaning or grouping to be useful.\n",
    "\n",
    "- **Overall Impression**\n",
    "  - Tweets are short and concise.\n",
    "  - Token length distribution is fairly symmetric.\n",
    "  - Primary signal will likely come from the `text` column, with optional engineered features from `keyword` and `location`.\n",
    "\n",
    "## Next Steps for Processing/Cleaning\n",
    "1. Lowercase all text.\n",
    "2. Replace URLs with `<URL>` and @mentions with `<USER>`.\n",
    "3. Strip `#` but keep the hashtag word (e.g., `#earthquake` → `earthquake`).\n",
    "4. Normalize punctuation (do not delete):\n",
    "   - Keep apostrophes (negations matter: “don’t” ≠ “do”).\n",
    "   - Collapse repeated punctuation (`!!!` → `!`) and extra whitespace.\n",
    "5. Unicode normalize (NFC) and trim leading/trailing spaces.\n",
    "6. Keep digits as-is (numbers often carry disaster-related meaning like counts, magnitudes, road IDs)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a75a59d9",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Cleaning & split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07127c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section summary — Cleaning:\n",
      "- Lowercased, replaced URLs and mentions with placeholders, kept hashtag words, normalized punctuation, collapsed spaces.\n",
      "- Created train/validation split (stratified).\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def basic_clean(s: str) -> str:\n",
    "    # Lowercase\n",
    "    s = s.lower()\n",
    "\n",
    "    # Replace URLs and @mentions\n",
    "    s = re.sub(r\"http\\S+|www\\.\\S+\", \" <url> \", s)\n",
    "    s = re.sub(r\"@\\w+\", \" <user> \", s)\n",
    "\n",
    "    # Keep hashtag words but strip '#'\n",
    "    s = re.sub(r\"#(\\w+)\", r\" \\1 \", s)\n",
    "\n",
    "    # Unicode normalize\n",
    "    s = unicodedata.normalize(\"NFC\", s)\n",
    "\n",
    "    # Collapse repeated punctuation (keep apostrophes)\n",
    "    s = re.sub(r\"([!?.,])\\1+\", r\"\\1\", s)  # !!! → !\n",
    "    \n",
    "    # Collapse extra whitespace\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "    return s\n",
    "\n",
    "# Apply cleaning\n",
    "train_df[\"text_clean\"] = train_df[\"text\"].astype(str).map(basic_clean)\n",
    "test_df[\"text_clean\"]  = test_df[\"text\"].astype(str).map(basic_clean)\n",
    "\n",
    "# Stratified split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_tmp, val_df = train_test_split(\n",
    "    train_df,\n",
    "    test_size=CFG[\"val_size\"],\n",
    "    stratify=train_df[\"target\"],\n",
    "    random_state=CFG[\"seed\"]\n",
    ")\n",
    "y_train = train_tmp[\"target\"].values\n",
    "y_val   = val_df[\"target\"].values\n",
    "\n",
    "print(\"Section summary — Cleaning:\")\n",
    "print(\"- Lowercased, replaced URLs and mentions with placeholders, kept hashtag words, normalized punctuation, collapsed spaces.\")\n",
    "print(\"- Created train/validation split (stratified).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f05ae3e",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Model architecture & rationale\n",
    "\n",
    "The task is to decide if a short tweet is about a **real disaster** or **not**.  \n",
    "Tweets are messy—full of hashtags, mentions, URLs, and informal language—so the model needs to capture both **content** and **word order**.\n",
    "\n",
    "We preprocess by tokenizing each tweet into integers, padding sequences to the same length, and passing them through a learned embedding layer that trains alongside the classification task. We avoid TF-IDF because it ignores word order, instead using a sequential neural network to capture both meaning and context.\n",
    "The embedding layer starts with random vectors for each token and updates them during training so that words used in similar contexts have similar vector representations.\n",
    "\n",
    "Our main model is a **Bidirectional LSTM**:\n",
    "- Reads the tweet forwards and backwards for richer context.\n",
    "- LSTM handles sequences well and remembers important parts.\n",
    "- We test two pooling approaches:\n",
    "  1. Last hidden state – full sequence summary.\n",
    "  2. Global Max Pooling – strongest features anywhere in the tweet.\n",
    "- A final dense sigmoid layer outputs the probability of a disaster.\n",
    "\n",
    "Training uses binary cross-entropy loss, Adam optimizer (LR = 1e-3 and 5e-4), early stopping, and learning-rate reduction. AUC is our main evaluation metric, with accuracy as a secondary check.\n",
    "\n",
    "This setup works well for tweets because they are short, making bidirectional processing fast, and because word order matters (“not a fire” ≠ “fire in the park”). Pooling comparisons will reveal whether overall context or key words matter more.\n",
    "\n",
    "We will also compare with a GRU model, adjust sequence length, hidden units, and dropout to explore model variations. From this we expect to learn how pooling affects performance, how to balance size and regularization, and whether GRU can match or beat LSTM. Future steps may include adding attention, using pretrained embeddings like GloVe, or testing a small transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b6fe27",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Functions — tokenize → pad, build BiLSTM/GRU, train & evaluate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa0012d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Section summary — Functions:\n",
      "- Tokenizer + padding to fixed sequence length.\n",
      "- Build BiLSTM/GRU with selectable pooling, units, dropout, and learning rate.\n",
      "- Train with EarlyStopping & ReduceLROnPlateau; report AUC/Accuracy.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score, precision_recall_curve\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "def fit_tokenizer(texts: pd.Series):\n",
    "    tok = keras.preprocessing.text.Tokenizer(num_words=None, oov_token=\"<oov>\")\n",
    "    tok.fit_on_texts(texts)\n",
    "    return tok\n",
    "\n",
    "def to_padded(tok, texts: pd.Series, max_len: int):\n",
    "    seqs = tok.texts_to_sequences(texts)\n",
    "    return keras.preprocessing.sequence.pad_sequences(seqs, maxlen=max_len)\n",
    "\n",
    "def build_rnn(cfg: Dict, vocab_size: int, *, cell=\"lstm\", pool=\"last\", units=None, dropout=None, lr=None):\n",
    "    units   = units   if units   is not None else cfg[\"units\"]\n",
    "    dropout = dropout if dropout is not None else cfg[\"dropout\"]\n",
    "    lr      = lr      if lr      is not None else cfg[\"learning_rate\"]\n",
    "\n",
    "    inputs = layers.Input(shape=(cfg[\"max_len\"],))\n",
    "    x = layers.Embedding(vocab_size+1, cfg[\"embedding_dim\"])(inputs)\n",
    "    RNN = layers.LSTM if cell==\"lstm\" else layers.GRU\n",
    "    x = layers.Bidirectional(RNN(units, return_sequences=(pool!=\"last\")))(x)\n",
    "    if dropout and dropout>0: x = layers.Dropout(dropout)(x)\n",
    "    if pool == \"gmp\": x = layers.GlobalMaxPooling1D()(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    m = keras.Model(inputs, outputs)\n",
    "    m.compile(optimizer=keras.optimizers.Adam(lr), loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\", keras.metrics.AUC(name=\"auc\")])\n",
    "    return m\n",
    "\n",
    "\n",
    "def train_and_eval(model, Xtr, ytr, Xva, yva, cfg: Dict):\n",
    "    callbacks = [\n",
    "        keras.callbacks.ReduceLROnPlateau(patience=2, factor=0.5, verbose=1),\n",
    "        keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True, verbose=1),\n",
    "    ]\n",
    "    hist = model.fit(\n",
    "        Xtr, ytr,\n",
    "        validation_data=(Xva, yva),\n",
    "        epochs=cfg[\"epochs\"],\n",
    "        batch_size=cfg[\"batch_size\"],\n",
    "        verbose=1,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    # Predict on validation set\n",
    "    y_prob = model.predict(Xva, batch_size=cfg[\"batch_size\"]).ravel()\n",
    "\n",
    "    # Find best threshold by F1 score\n",
    "    prec, rec, thr = precision_recall_curve(yva, y_prob)\n",
    "    f1s = 2 * prec * rec / (prec + rec + 1e-12)\n",
    "    best_idx = f1s[:-1].argmax() if len(thr) else 0\n",
    "    best_t = float(thr[best_idx]) if len(thr) else 0.5\n",
    "\n",
    "    # Apply best threshold\n",
    "    y_pred = (y_prob >= best_t).astype(int)\n",
    "\n",
    "    # Compute metrics\n",
    "    acc = accuracy_score(yva, y_pred)\n",
    "    auc = roc_auc_score(yva, y_prob)\n",
    "    f1  = f1_score(yva, y_pred)\n",
    "\n",
    "    # Print summary line\n",
    "    print(f\"Best F1={f1:.4f} at threshold={best_t:.3f} | AUC={auc:.4f} ACC={acc:.4f}\")\n",
    "\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"auc\": auc,\n",
    "        \"f1\": f1,\n",
    "        \"best_t\": best_t,\n",
    "        \"history\": hist.history,\n",
    "    }\n",
    "\n",
    "print(\"Section summary — Functions:\")\n",
    "print(\"- Tokenizer + padding to fixed sequence length.\")\n",
    "print(\"- Build BiLSTM/GRU with selectable pooling, units, dropout, and learning rate.\")\n",
    "print(\"- Train with EarlyStopping & ReduceLROnPlateau; report AUC/Accuracy.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab6d6d4",
   "metadata": {},
   "source": [
    "\n",
    "## 7) Quick sanity run (small BiLSTM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ca8587a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 16:11:49.180652: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1886] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 43694 MB memory:  -> device: 0, name: NVIDIA A40, pci bus id: 0000:d2:00.0, compute capability: 8.6\n",
      "2025-08-12 16:11:49.322633: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-12 16:11:52.724236: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:442] Loaded cuDNN version 8905\n",
      "2025-08-12 16:11:55.037342: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x74ec00499ed0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2025-08-12 16:11:55.037380: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA A40, Compute Capability 8.6\n",
      "2025-08-12 16:11:55.045828: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2025-08-12 16:11:55.154296: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203/203 [==============================] - 36s 150ms/step - loss: 0.5340 - accuracy: 0.7336 - auc: 0.7893 - val_loss: 0.4415 - val_accuracy: 0.8117 - val_auc: 0.8682 - lr: 0.0010\n",
      "Epoch 2/6\n",
      "203/203 [==============================] - 11s 56ms/step - loss: 0.3056 - accuracy: 0.8781 - auc: 0.9324 - val_loss: 0.4539 - val_accuracy: 0.8082 - val_auc: 0.8597 - lr: 0.0010\n",
      "Epoch 3/6\n",
      "203/203 [==============================] - ETA: 0s - loss: 0.1904 - accuracy: 0.9328 - auc: 0.9732\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "203/203 [==============================] - 8s 39ms/step - loss: 0.1904 - accuracy: 0.9328 - auc: 0.9732 - val_loss: 0.5487 - val_accuracy: 0.7741 - val_auc: 0.8448 - lr: 0.0010\n",
      "Epoch 4/6\n",
      "203/203 [==============================] - ETA: 0s - loss: 0.1091 - accuracy: 0.9641 - auc: 0.9905Restoring model weights from the end of the best epoch: 1.\n",
      "203/203 [==============================] - 8s 38ms/step - loss: 0.1091 - accuracy: 0.9641 - auc: 0.9905 - val_loss: 0.6082 - val_accuracy: 0.7671 - val_auc: 0.8422 - lr: 5.0000e-04\n",
      "Epoch 4: early stopping\n",
      "36/36 [==============================] - 1s 5ms/step\n",
      "Best F1=0.7747 at threshold=0.369 | AUC=0.8680 ACC=0.8004\n",
      "Sanity run — AUC: 0.8680394567655589 ACC: 0.8003502626970228\n",
      "Section summary — Sanity run:\n",
      "- Verified end‑to‑end flow on small BiLSTM.\n",
      "- Proceed to staged HPO.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "CFG[\"max_len\"] = 40\n",
    "tok = fit_tokenizer(train_tmp[\"text_clean\"])\n",
    "Xtr = to_padded(tok, train_tmp[\"text_clean\"], CFG[\"max_len\"])\n",
    "Xva = to_padded(tok, val_df[\"text_clean\"],   CFG[\"max_len\"])\n",
    "\n",
    "model = build_rnn(CFG, vocab_size=len(tok.word_index), cell=\"lstm\", pool=\"last\", units=64, dropout=0.2, lr=1e-3)\n",
    "metrics0 = train_and_eval(model, Xtr, y_train, Xva, y_val, CFG)\n",
    "print(\"Sanity run — AUC:\", metrics0[\"auc\"], \"ACC:\", metrics0[\"acc\"])\n",
    "\n",
    "print(\"Section summary — Sanity run:\\n- Verified end‑to‑end flow on small BiLSTM.\\n- Proceed to staged HPO.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e262f09d",
   "metadata": {},
   "source": [
    "\n",
    "## 8) HPO — Stage A: structure (max_len × pooling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2165395e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results = []\n",
    "for max_len, pool in [(40,\"last\"), (40,\"gmp\"), (60,\"last\"), (60,\"gmp\")]:\n",
    "    CFG[\"max_len\"] = max_len\n",
    "    tok = fit_tokenizer(train_tmp[\"text_clean\"])\n",
    "    Xtr = to_padded(tok, train_tmp[\"text_clean\"], max_len)\n",
    "    Xva = to_padded(tok, val_df[\"text_clean\"],   max_len)\n",
    "    model = build_rnn(CFG, vocab_size=len(tok.word_index), cell=\"lstm\", pool=pool, units=64, dropout=0.2, lr=1e-3)\n",
    "    m = train_and_eval(model, Xtr, y_train, Xva, y_val, CFG)\n",
    "    results.append({\"stage\":\"A\",\"max_len\":max_len,\"pool\":pool,\"units\":64,\"lr\":1e-3,\"dropout\":0.2,\"auc\":m[\"auc\"],\"acc\":m[\"acc\"]})\n",
    "\n",
    "df_results = pd.DataFrame(results).sort_values(\"auc\", ascending=False)\n",
    "display(df_results.head(10))\n",
    "\n",
    "print(\"Section summary — Stage A:\\n- Compared max_len {40,60} and pooling {last,GMP}.\\n- Keeping the best combo by validation AUC.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910c86c3",
   "metadata": {},
   "source": [
    "\n",
    "## 9) HPO — Stage B: capacity × learning rate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bb691e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bestA = df_results.iloc[0].to_dict()\n",
    "best_max_len, best_pool = int(bestA[\"max_len\"]), bestA[\"pool\"]\n",
    "CFG[\"max_len\"] = best_max_len\n",
    "\n",
    "tok = fit_tokenizer(train_tmp[\"text_clean\"])\n",
    "Xtr = to_padded(tok, train_tmp[\"text_clean\"], best_max_len)\n",
    "Xva = to_padded(tok, val_df[\"text_clean\"],   best_max_len)\n",
    "\n",
    "for units in [64, 128]:\n",
    "    for lr in [1e-3, 5e-4]:\n",
    "        model = build_rnn(CFG, vocab_size=len(tok.word_index), cell=\"lstm\", pool=best_pool, units=units, dropout=0.2, lr=lr)\n",
    "        m = train_and_eval(model, Xtr, y_train, Xva, y_val, CFG)\n",
    "        results.append({\"stage\":\"B\",\"max_len\":best_max_len,\"pool\":best_pool,\"units\":units,\"lr\":lr,\"dropout\":0.2,\"auc\":m[\"auc\"],\"acc\":m[\"acc\"]})\n",
    "\n",
    "df_results = pd.DataFrame(results).sort_values([\"stage\",\"auc\"], ascending=[True, False])\n",
    "display(df_results[df_results[\"stage\"]==\"B\"].sort_values(\"auc\", ascending=False))\n",
    "\n",
    "print(\"Section summary — Stage B:\\n- Tuned units {64,128} and lr {1e‑3, 5e‑4} on best structure.\\n- Select best by AUC.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f6347b",
   "metadata": {},
   "source": [
    "\n",
    "## 10) HPO — Stage C: regularization (dropout)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac43189",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_overall = pd.DataFrame(results).sort_values(\"auc\", ascending=False).iloc[0].to_dict()\n",
    "CFG[\"max_len\"] = int(best_overall[\"max_len\"])\n",
    "\n",
    "tok = fit_tokenizer(train_tmp[\"text_clean\"])\n",
    "Xtr = to_padded(tok, train_tmp[\"text_clean\"], CFG[\"max_len\"])\n",
    "Xva = to_padded(tok, val_df[\"text_clean\"],   CFG[\"max_len\"])\n",
    "\n",
    "for dp in [0.0, 0.3]:\n",
    "    model = build_rnn(CFG, vocab_size=len(tok.word_index), cell=\"lstm\",\n",
    "                      pool=best_overall[\"pool\"], units=int(best_overall[\"units\"]), lr=float(best_overall[\"lr\"]), dropout=dp)\n",
    "    m = train_and_eval(model, Xtr, y_train, Xva, y_val, CFG)\n",
    "    results.append({\"stage\":\"C\",\"max_len\":CFG[\"max_len\"],\"pool\":best_overall[\"pool\"],\n",
    "                    \"units\":int(best_overall[\"units\"]),\"lr\":float(best_overall[\"lr\"]),\"dropout\":dp,\"auc\":m[\"auc\"],\"acc\":m[\"acc\"]})\n",
    "\n",
    "df_results = pd.DataFrame(results).sort_values(\"auc\", ascending=False)\n",
    "display(df_results.head(10))\n",
    "\n",
    "print(\"Section summary — Stage C:\\n- Tested dropout {0.0, 0.3} on the current best.\\n- Best config fixed for final training and submission.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2d543b",
   "metadata": {},
   "source": [
    "\n",
    "## 11) (Optional) GRU variant\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121fa211",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_now = pd.DataFrame(results).sort_values(\"auc\", ascending=False).iloc[0].to_dict()\n",
    "CFG[\"max_len\"] = int(best_now[\"max_len\"])\n",
    "\n",
    "tok = fit_tokenizer(train_tmp[\"text_clean\"])\n",
    "Xtr = to_padded(tok, train_tmp[\"text_clean\"], CFG[\"max_len\"])\n",
    "Xva = to_padded(tok, val_df[\"text_clean\"],   CFG[\"max_len\"])\n",
    "\n",
    "model = build_rnn(CFG, vocab_size=len(tok.word_index), cell=\"gru\",\n",
    "                  pool=best_now[\"pool\"], units=int(best_now[\"units\"]), lr=float(best_now[\"lr\"]), dropout=float(best_now[\"dropout\"]))\n",
    "m = train_and_eval(model, Xtr, y_train, Xva, y_val, CFG)\n",
    "results.append({\"stage\":\"D_gru\",\"max_len\":CFG[\"max_len\"],\"pool\":best_now[\"pool\"],\n",
    "                \"units\":int(best_now[\"units\"]),\"lr\":float(best_now[\"lr\"]),\"dropout\":float(best_now[\"dropout\"]),\n",
    "                \"auc\":m[\"auc\"], \"acc\":m[\"acc\"]})\n",
    "\n",
    "df_results = pd.DataFrame(results).sort_values(\"auc\", ascending=False)\n",
    "display(df_results.head(10))\n",
    "\n",
    "print(\"Section summary — GRU:\\n- Compared GRU with best hyperparams.\\n- Provides an architecture contrast within the same family.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d02b48",
   "metadata": {},
   "source": [
    "\n",
    "## 12) Results & analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6317c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "h2(\"Top results\")\n",
    "display(pd.DataFrame(results).sort_values(\"auc\", ascending=False).head(10))\n",
    "\n",
    "# Optional: plot learning curve for the best run if you saved it\n",
    "print(\"Section summary — Results:\\n- Reported best AUC and Accuracy across small grids.\\n- Observed the effect of pooling, sequence length, units, lr, and dropout.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed388b2",
   "metadata": {},
   "source": [
    "\n",
    "## 13) Train best on full train → predict test → submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfb3d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best = pd.DataFrame(results).sort_values(\"auc\", ascending=False).iloc[0].to_dict()\n",
    "\n",
    "# Refit on full train\n",
    "CFG[\"max_len\"] = int(best[\"max_len\"])\n",
    "tok_full = fit_tokenizer(train_df[\"text_clean\"])\n",
    "Xfull = to_padded(tok_full, train_df[\"text_clean\"], CFG[\"max_len\"])\n",
    "yfull = train_df[\"target\"].values\n",
    "\n",
    "model = build_rnn(CFG, vocab_size=len(tok_full.word_index), cell=\"lstm\",\n",
    "                  pool=best[\"pool\"], units=int(best[\"units\"]), lr=float(best[\"lr\"]), dropout=float(best[\"dropout\"]))\n",
    "_ = train_and_eval(model, Xfull, yfull, Xfull[:256], yfull[:256], CFG)  # quick sanity; can be removed on GPU\n",
    "\n",
    "# Predict test & save\n",
    "Xtest = to_padded(tok_full, test_df[\"text_clean\"], CFG[\"max_len\"])\n",
    "ytest_prob = model.predict(Xtest, batch_size=CFG[\"batch_size\"]).ravel()\n",
    "sub = pd.DataFrame({\"id\": test_df[\"id\"], \"target\": (ytest_prob >= 0.5).astype(int)})\n",
    "sub.to_csv(CFG[\"paths\"][\"submission\"], index=False)\n",
    "print(\"Saved submission:\", CFG[\"paths\"][\"submission\"])\n",
    "\n",
    "print(\"Section summary — Submission:\\n- Trained best setting on full train and generated Kaggle submission CSV.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb85d0b",
   "metadata": {},
   "source": [
    "\n",
    "## 14) Conclusion\n",
    "- Bidirectional LSTM with a learned embedding is effective for short, noisy tweets.\n",
    "- Pooling choice and sequence length have measurable impact; a small grid sufficed to find a strong setting.\n",
    "- Future work: simple attention for interpretability; pretrained embeddings (GloVe); compact transformer baseline.\n",
    "\n",
    "\n",
    "## 15) References\n",
    "- Brownlee, J. (2021). *How to Use Word Embedding Layers for Deep Learning with Keras*. Machine Learning Mastery. [https://www.machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/](https://www.machinelearningmastery.com/use-word-embedding-layers-deep-learning-keras/)\n",
    "\n",
    "- GeeksforGeeks. (2025). *Bidirectional LSTM in NLP*. [https://www.geeksforgeeks.org/nlp/bidirectional-lstm-in-nlp/](https://www.geeksforgeeks.org/nlp/bidirectional-lstm-in-nlp/)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
